[{"authors":["admin"],"categories":null,"content":"","date":1600092000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1600092000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tomekent.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Thomas E. Kent","type":"authors"},{"authors":["Anas"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5bb6fedbe949f22d589566cbd6b28a96","permalink":"https://tomekent.com/authors/anas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/anas/","section":"authors","summary":"","tags":null,"title":"Anas Shrinah","type":"authors"},{"authors":["Christos"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"37c53f8201b3281d1bd36c3b71af2d9a","permalink":"https://tomekent.com/authors/christos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/christos/","section":"authors","summary":"","tags":null,"title":"Christos Sevastopoulos","type":"authors"},{"authors":["Name \"Jian Jiao\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"c00edd18cddb5f26a820adb80e3beacd","permalink":"https://tomekent.com/authors/jian-jiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jian-jiao/","section":"authors","summary":"","tags":null,"title":"Jian Jiao","type":"authors"},{"authors":["Karam"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3023669eefccd6e7fc3f6b1033dc90b6","permalink":"https://tomekent.com/authors/karam/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/karam/","section":"authors","summary":"","tags":null,"title":"Karam Safarov","type":"authors"},{"authors":["Name \"Xingyu Guo\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"38c19d51b74244e598be3340216f491c","permalink":"https://tomekent.com/authors/xingyu-guo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xingyu-guo/","section":"authors","summary":"","tags":null,"title":"Xingyu Guo","type":"authors"},{"authors":["Thomas E. Kent"],"categories":null,"content":"","date":1600092000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600092000,"objectID":"6fbb9be09763d1f745ed4419bcbf0263","permalink":"https://tomekent.com/talk/eumas2020/","publishdate":"2020-09-14T10:39:44+01:00","relpermalink":"/talk/eumas2020/","section":"talk","summary":"Modelling and planning as well as Machine Learning techniques such as Reinforcement Learning are often difficult in multi-agent problems. With increasing numbers of agents the decision space grows rapidly and is made increasingly complex through interacting agents. This paper is motivated by the question of if it is possible to train single- agent policies in isolation and without the need for explicit cooperation or coordination still successfully deploy them to multi-agent scenarios. In particular we look at the multi-agent Persistent Surveillance Problem (MAPSP), which is the problem of using a number of agents to continually visit and re-visit areas of a map to maximise a metric of surveillance. We outline five distinct single-agent policies to solve the MAPSP: Reinforcement Learning (DDPG); Neuro-Evolution (NEAT); a Gradient Descent (GD) heuristic; a random heuristic; and a pre-defined ‘ploughing pattern’ (Trail). We will compare the performance and scalability of these single-agent policies to the Multi-Agent PSP. Importantly, in doing so we will demonstrate an emergent property which we call the Homogeneous-Policy Convergence Cycle (HPCC), whereby agents following homogeneous policies can get stuck together, continuously repeating the same action as other agents, significantly impacting performance. This paper will show that just a small amount of noise, at the state or action level, is sufficient to solve the problem, essentially creating artificially-heterogeneous policies for the agents.","tags":[],"title":"Single-Agent Policies for the Multi-Agent Persistent Surveillance Problem via Artificial Heterogeneity","type":"talk"},{"authors":["Thomas Kent","Arthur Richards","Angus Johnson"],"categories":[],"content":"","date":1600079885,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600079885,"objectID":"825c5ac25705253149f6d83f6aecf9be","permalink":"https://tomekent.com/publication/kent-2020a/","publishdate":"2020-09-14T11:38:05+01:00","relpermalink":"/publication/kent-2020a/","section":"publication","summary":"Modelling and planning as well as Machine Learning techniques such as Reinforcement Learning are often difficult in multi-agent problems. With increasing numbers of agents the decision space grows rapidly and is made increasingly complex through interacting agents. This paper is motivated by the question of if it is possible to train single- agent policies in isolation and without the need for explicit cooperation or coordination still successfully deploy them to multi-agent scenarios. In particular we look at the multi-agent Persistent Surveillance Problem (MAPSP), which is the problem of using a number of agents to continually visit and re-visit areas of a map to maximise a metric of surveillance. We outline five distinct single-agent policies to solve the MAPSP: Reinforcement Learning (DDPG); Neuro-Evolution (NEAT); a Gradient Descent (GD) heuristic; a random heuristic; and a pre-defined ‘ploughing pattern’ (Trail). We will compare the performance and scalability of these single-agent policies to the Multi-Agent PSP. Importantly, in doing so we will demonstrate an emergent property which we call the Homogeneous-Policy Convergence Cycle (HPCC), whereby agents following homogeneous policies can get stuck together, continuously repeating the same action as other agents, significantly impacting performance. This paper will show that just a small amount of noise, at the state or action level, is sufficient to solve the problem, essentially creating artificially-heterogeneous policies for the agents.","tags":[],"title":"Single-Agent Policies for the Multi-Agent Persistent Surveillance Problem via Artificial Heterogeneity","type":"publication"},{"authors":[],"categories":[],"content":"Title Author Name\n Python \u0026hellip; Hey\nif x\u0026lt;x_lim: run_me(x) else: run_you(x)   #header-left { position: absolute; top: 0%; left: 0%; } #header-right { position: absolute; top: 0%; right: 0%; } #footer-left { position: absolute; bottom: 0%; left: 0%; }   \u0026lt;div id=\u0026quot;hidden\u0026quot; style=\u0026quot;display:none;\u0026quot;\u0026gt; \u0026lt;div id=\u0026quot;header\u0026quot;\u0026gt; \u0026lt;div id=\u0026quot;header-left\u0026quot;\u0026gt;…\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;header-right\u0026quot;\u0026gt;…\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;footer-left\u0026quot;\u0026gt;…\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;reveal\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;slides\u0026quot;\u0026gt; … \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026quot;reveal.js/lib/js/head.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;reveal.js/js/reveal.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; … \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;jquery/jquery-2.1.3.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt; var header = $('#header').html(); if ( window.location.search.match( /print-pdf/gi ) ) { $('.slides \u0026gt; section').prepend(header); } else { $('.slides').prepend(header); } \u0026lt;/script\u0026gt;   ","date":1582710365,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582710365,"objectID":"c4f5c7a2264f9673a2de022adf50d111","permalink":"https://tomekent.com/slides/test/","publishdate":"2020-02-26T09:46:05Z","relpermalink":"/slides/test/","section":"slides","summary":"Title Author Name\n Python \u0026hellip; Hey\nif x\u0026lt;x_lim: run_me(x) else: run_you(x)   #header-left { position: absolute; top: 0%; left: 0%; } #header-right { position: absolute; top: 0%; right: 0%; } #footer-left { position: absolute; bottom: 0%; left: 0%; }   \u0026lt;div id=\u0026quot;hidden\u0026quot; style=\u0026quot;display:none;\u0026quot;\u0026gt; \u0026lt;div id=\u0026quot;header\u0026quot;\u0026gt; \u0026lt;div id=\u0026quot;header-left\u0026quot;\u0026gt;…\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;header-right\u0026quot;\u0026gt;…\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;footer-left\u0026quot;\u0026gt;…\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;reveal\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;slides\u0026quot;\u0026gt; … \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026quot;reveal.js/lib/js/head.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;reveal.js/js/reveal.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; … \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;jquery/jquery-2.","tags":[],"title":"Test","type":"slides"},{"authors":[],"categories":[],"content":"Never work with angles I\u0026rsquo;m sure if W.C. Fields was alive today and interested in machine learning he might have updated his maxim of never working with children or animals to include angles.\nMany off the shelf RL algorithms seem like straightforward tools to replicate state of the art results on your own specific problem domain. Deep neural nets have shown great promise of \u0026lsquo;learning\u0026rsquo; complex tasks such as computer vision and continuous control problems but to adapt them can be challenging and require greater knowledge of the underlying mathematics and Neural Network structures to debug problems - often parameter tuning by trial and error.\nLets have a look at Deep Deterministic Policy Gradient (DDPG) and see how well a Deep Neural Network, approximating a policy function, can perform at an angle based continuous control problem.\nContinuous Action Control choice for Persistent Surveillance Some of my current work is looking at training/designing polices for multiple agents to perform persistent surveillance (similar to coverage/patrolling problems.). The aim is for agents to maximise some metric, \u0026lsquo;hex score\u0026rsquo;, that represents how well the world\u0026rsquo;s current \u0026lsquo;level of surveillance\u0026rsquo;.\nAs an agent enters a hex it observes it and the score shoots up, then all hexes currently not directly observed have their score decay (with some half life).\n  Reinforcement Learning state, action reward flow   Our observation, and thus our input to our Neural Network are the hex scores around the agent. This is feed into our network in order to choose the optimal action (direction to move) for the agent to take.\nSingle Neuron Output The standard off the shelf DDPG algorithm for a continuous control task has a single neuron output.   Deep Neural Net with 1 output neuron   We replicate this and choose our output to be a single neuron, with a $\\tanh$ activation function. This activation value which lies between $[-1, 1]$ can be multiplied by $\\pi$ to recover an angle $\\theta \\in [-\\pi, +\\pi]$ - this is the direction the agent should move.   Activation function $\\tanh$   Trained DDPG Agent After 2000 runs of 600 time steps the DDPG policy has successfully converged and produces half decent results. It runs around heating those hexes up reaching decent scores, so lets see if we can improve it.\nRose Plots and Neuron Saturation While it appears to work fairly well, if we use a rose-plot to show action choices over an entire episode we can see an issue. This rose plot indicates frequency of angle chosen:   Action choices rose plot for 3 policies    On the left we have completely random action selection; In the middle a discrete 6 direction choice made by a simple greedy heuristic algorithm; On the right a Deep Deterministic Policy Gradient (DDPG) as outline above.  What we hope for is something that produces a better, more continuous version of the heuristic (middle), instead what we get is good old neuron saturation. The problem here is the $\\tanh$ activation function (and a problem that exists in all activation functions) saturates at +1 or -1, the asymptotes, and as a result so too does our action selection at $+\\pi$ or $-\\pi$ respectively.\nDouble Neuron Output That said, issues with working with angles and its discontinuities are well documented. One approach to fix this is to add more output neurons. We opt to add a second neuron and parametrise the action angle so that instead of multiplying the activation by $\\pi$ we use it to represent $\\sin(\\theta)$ and $\\cos(\\theta)$ (which are easily converted into $\\theta =\\arctan(\\sin(\\theta)/\\cos(\\theta))$).   Deep Neural Net with 2 output neuron     Action choices rose plot: [Random, discrete heuristic, DDPG]   This balances out some of the neuron saturation but also seems to instead now saturate at the \u0026lsquo;corners\u0026rsquo; [(1,1), (1,-1), (-1,-1), (-1,1)]\nMulti Agent Deployment Some of our most recent work looks at whether we can simply train single agent policies like above and deploy them to multi agent problems - without Multi-Agent RL. It turns out we can, but there are catches. I recently gave a talk called Ignorance is bliss - the role of noise and Heterogeneity in training and deployment of Single Agent Policies for the Multi-Agent Persistent Survellance Problem7 which goes into some more detail.\nA paper which is currently under review, Single-Agent Polices for the Multi-Agent Persistent Surveillance Problem via Artificial Heterogeneity, describes how deploying multiple heterogeneous agents can cause undesirable emergent clustering behaviour. So stay tuned for that.\n","date":1582200603,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582200603,"objectID":"da90bb2f256d9e6c304bb6b078631448","permalink":"https://tomekent.com/post/neuron_saturation/","publishdate":"2020-02-20T12:10:03Z","relpermalink":"/post/neuron_saturation/","section":"post","summary":"Never work with angles I\u0026rsquo;m sure if W.C. Fields was alive today and interested in machine learning he might have updated his maxim of never working with children or animals to include angles.\nMany off the shelf RL algorithms seem like straightforward tools to replicate state of the art results on your own specific problem domain. Deep neural nets have shown great promise of \u0026lsquo;learning\u0026rsquo; complex tasks such as computer vision and continuous control problems but to adapt them can be challenging and require greater knowledge of the underlying mathematics and Neural Network structures to debug problems - often parameter tuning by trial and error.","tags":[],"title":"Continuous Control, DDPG and Neuron saturation","type":"post"},{"authors":[],"categories":[],"content":"This is a summary of some work published last year at the 2019 Genetic and Evolutionary Computation Conference . Where we are looking to solve routing and allocation problems and trying to find ways to balance the increase in computational demand required when increasing the number of numbers of agents\n  Trade-off between optimisation factors   If we want to solve increasingly complex problems, then we might either need more compute, or we sacrifice solution quality - unless we can be smart about the way we decompose the problem.\nGeneral Problem There are a number of real world problems that require several agents to visit areas of interest, complete tasks and travel between them. These typically include problems such as surveillance, exploration or search and rescue.\nHere we define a fairly standard Multi-Agent Travelling Salesman Problem:\nSetup  Allocating active set of tasks to a set of agents Simultaneously planning agents\u0026rsquo; optimal routes  Caveats  Allocation and routing are closely coupled Dynamic simulated environment Need to consider real world implementation  What we have learnt from our work on this is that:\n We should align real-world Multi-Agent constraints with the structuring of our optimisation technique\n Evolutionary Algorithm for MATSP Our aim is to move away from a single centralised solution to the MATSP. Instead we want to utilise the fact that these agents exist in a world that can be compartmentalised. There are real world constraints, such as geographical seperation that can be used to our advantage.\nWe use the population-distribution island-model, where the global population is divided into a number of demes (distinct populations) and referred to as the Multi-Demic Evolutionary Algorithm (MDEA). Communications between these demes allow for individuals to migrate between them at pre-defined intervals. These demes are structured to align with real world execution of a MATSP where tasks are distributed amongst multiple agents and are completed independently.\nMATSP Problem Statement Here we present what\u0026rsquo;s known as thethe three-index flow-based formulation First define the indexes $i$ and $j$ to denote a task from the set T of tasks 1 to N , the set A of agents from 1 to M and the matrix c i ja to denote the cost of agent a travelling from task i to task j. Additionally we define the three-index binary decision variable:\nFirst define the indexes $i$ and $j$ to denote a task from the set $T$ of tasks 1 to $N$, the set $A$ of agents from 1 to $M$ and the matrix $c_{ija}$ to denote the cost of agent $a$ travelling from task $i$ to task $j$. Additionally we define the three-index binary decision variable:\n$$ \\begin{equation*} x_{ija} = \\begin{cases} 1 \u0026amp; \\text{if agent $a$ visits task $j$ after $i$,}\\\\\\\n0 \u0026amp; \\text{otherwise} \\end{cases} \\end{equation*} $$\n$$ \\begin{align} \\min_{x_{ija}} \u0026amp; \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} \\sum_{a = 1}^{M} c_{ija} x_{ija} \\\\\\\n\u0026amp; \\sum_{i = 1}^{N}x_{ipa} - \\sum_{j = 1}^{N}x_{pja} = 0, \\text{ }a \\in A, p \\in T \\label{MATSP_flow} \\\\\\\n\u0026amp; \\sum_{j = 1}^{N}x_{1ja} = 1, \\text{ } \\forall a \\in A \\label{MATSP_one_agent}\\\\\\\n\u0026amp; u_i - u_j + N \\sum_{a = 1}^{M} x_{ija} \\leq N -1, \\text{ } \\forall i \\neq j \\neq 1 \\label{MATSP_subtour}\\\\\\\n\u0026amp; x_{ija} \\in {0,1} \\text{ } \\forall i,j,a \\end{align} $$\nThe objective, is to minimize the total cost of all the agents travelling between the assigned tasks. The next constraints ensure that each task is visited only once while the flow conservation constraints state that once an agent visits a task then they must also depart from it. The \u0026lsquo;one-agent\u0026rsquo; constraints ensure that each agent is used only once and the sub-tour elimination constraints are used, where $u$ are additional non-negative auxiliary decision variables, with $u_i$ corresponding to the ith task, known as \u0026lsquo;node potentials\u0026rsquo;.\nEvolutionary Approach for MATSP 3 main Evolutionary Stages  Initialisation - creating an starting population for which to evolve; Reproduction - carrying out evolutionary operators such as crossover, mutation and improvement heuristics to produce offspring; Selection - taking individuals from both the main population and from the offspring to produce the new population;     Chromosome representation of route structure   Crossover operator to create offspring from combining features of the two parent solutions   SBX Crossover operator   Mutation operators to create slightly different offspring from one parent solutions     Mutation operator - reallocate tasks     Mutation operator - Swap Tasks   Improvement operator to try to quickly improve a solution using 2 opt   Improvement operator - 2 opt swap   Aim: Multi-demic Evolutionary Algorithm  Exploit problem structure, aligning real-world implementation demands Decentralised solution with Communication Use multiple populations (or demes) With well-defined agent-population interactions  Each agent has population structures for \u0026lsquo;thinking about\u0026rsquo; intereactions with other agents. A population \u0026lsquo;A-B\u0026rsquo; that exists onboard agent A is constantly evolved and updated. If new tasks arise these are incorporated, if agent A and agent B come in communiction range then they can \u0026lsquo;pool\u0026rsquo; their A-B and B-A populations and prune any that are now invalid.\n  Population Structures   Results The objective function, total distance travelled, shown in the below figure, clearly shows that as the communications restriction is gradually lifted the total distances of the dMDEA results tends to the cMDEA, notably, any communication radius of 125 or greater either matches or outperforms the EA.\n  Communication range vs Total Distance   In addition, as communication range is increased the agents spend more time evolving the demes corresponding to nearby agents and thus the linear runtime increases. The next figure shows the relationship between the communication radius and thus the number of other agents to consider and the resulting run-time. Therefore there is a clear trade-off decision between ability to communicate, and thus agents you should consider, and run-time.\n  Communication range vs Total Sim time   Demos Decentralised - Homogeneous Comms   Decentralised - Heterogeneous Comms   But I was after a free lunch   Trade-off between optimisation factors   Sadly that\u0026rsquo;s really the issue here, in many optimisation problems, but particularly in heuristic iterative approaches such as EAs, there is a trade off between problem complexity, runtime and quality of solution (Figure 4). If you want to keep your compute time the same, i.e. limit generations and population size, but need to solve a harder problem, i.e. one with more agents, you will need to sacrifice solution quality.\n  Trade-off between optimisation factors   Read the full paper If you want to read in a bit more detail then you can find the full paper here:  Kent 2019  ","date":1581116139,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581116139,"objectID":"c6836c7bd21463bee0d4a4f9f941fd12","permalink":"https://tomekent.com/post/multiagent-travellingsalesmanproblem/","publishdate":"2020-02-07T22:55:39Z","relpermalink":"/post/multiagent-travellingsalesmanproblem/","section":"post","summary":"This is a summary of some work published last year at the 2019 Genetic and Evolutionary Computation Conference . Where we are looking to solve routing and allocation problems and trying to find ways to balance the increase in computational demand required when increasing the number of numbers of agents\n  Trade-off between optimisation factors   If we want to solve increasingly complex problems, then we might either need more compute, or we sacrifice solution quality - unless we can be smart about the way we decompose the problem.","tags":[],"title":"Evolutionary Algorithms for the Multi Agent Travelling Salesman Problem","type":"post"},{"authors":null,"categories":null,"content":"Co2 Emissions I had a quick look back over some of my PhD work in preparation of a new paper I\u0026rsquo;m writing. I found it slightly mad that I never really included any numbers on Co2 emissions.\nSo I\u0026rsquo;ve been working on some under the assumptions:\n The emissions index of CO 2 per kg of fuel burnt is estimated to be roughly 3.16 [1]  A rough estimate of 600 dollars per metric tonne of fuel [2]   Singapore Airlines Take Singapore Airlines as an example, on a single day some 200 flights could potentially fly in 100 formations pairs. This would save on average 6.7% of fuel against those same flights flying solo. This works out at 610 tonnes of fuel saved, and as a direct result would burn roughly 1,929 tonnes fewer of Co2. Not only that but they\u0026rsquo;d save 366k in the process but simply not buying the fuel.\nThat\u0026rsquo;s for a single day.\nThat\u0026rsquo;s like planting 704,085 trees a year and getting paid $133m for it.\nEquivalent of planting 80 per hour: 🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳\nAnd getting paid $190 per tree at the same time 💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰\nWant to find out more? Take a look a page I put together a while back to explore the datasets: http://tomekent.com/FormationFlight/\nFound this interesting? Consider sharing it 🙌 ","date":1579046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579046400,"objectID":"775ed51acdb6bc498fb63f0023c7d706","permalink":"https://tomekent.com/post/co2-formation/","publishdate":"2020-01-15T00:00:00Z","relpermalink":"/post/co2-formation/","section":"post","summary":"Co2 Emissions I had a quick look back over some of my PhD work in preparation of a new paper I\u0026rsquo;m writing. I found it slightly mad that I never really included any numbers on Co2 emissions.\nSo I\u0026rsquo;ve been working on some under the assumptions:\n The emissions index of CO 2 per kg of fuel burnt is estimated to be roughly 3.16 [1]  A rough estimate of 600 dollars per metric tonne of fuel [2]   Singapore Airlines Take Singapore Airlines as an example, on a single day some 200 flights could potentially fly in 100 formations pairs.","tags":null,"title":"Co2 Emissions Reduction via formation flight","type":"post"},{"authors":["Thomas Kent","Anthony Pipe","Arthur Richards","Jim Hutchinson","Wolfgang Schuster"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3af7f1e8fe9a45be5e93fa8cd2767fdb","permalink":"https://tomekent.com/publication/kent-2020/","publishdate":"2020-06-23T15:25:33.125428Z","relpermalink":"/publication/kent-2020/","section":"publication","summary":"VENTURER was one of the first three UK government funded research and innovation projects on Connected Autonomous Vehicles (CAVs) and was conducted predominantly in the South West region of the country. A series of increasingly complex scenarios conducted in an urban setting were used to: (i) evaluate the technology created as a part of the project; (ii) systematically assess participant responses to CAVs and; (iii) inform the development of potential insurance models and legal frameworks. Developing this understanding contributed key steps towards facilitating the deployment of CAVs on UK roads. This paper aims to describe the VENTURER Project trials, their objectives and detail some of the key technologies used. Importantly we aim to introduce some informative challenges that were overcame and the subsequent project and technological lessons learned in a hope to help others plan and execute future CAV research. The project successfully integrated several technologies crucial to CAV development. These included, a Decision Making System using behaviour trees to make high level decisions; A pilot-control system to smoothly and comfortably turn plans into throttle and steering actuation; Sensing and perception systems to make sense of raw sensor data; Inter-CAV Wireless communication capable of demonstrating vehicle-to-vehicle communication of potential hazards. The closely coupled technology integration, testing and participant-focused trial schedule led to a greatly improved understanding of the engineering and societal barriers that CAV development faces. From a behavioural standpoint the importance of reliability and repeatability far outweighs a need for novel trajectories, while the sensor-to-perception capabilities are critical, the process of verification and validation is extremely time consuming.","tags":null,"title":"A Connected Autonomous Vehicle Testbed: Capabilities, Experimental Processes and Lessons Learned","type":"publication"},{"authors":["Thomas E. Kent"],"categories":null,"content":"","date":1571497200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571497200,"objectID":"0444ea1284f766dfa52555f5e407cb1f","permalink":"https://tomekent.com/talk/collectivedynamics/","publishdate":"2019-10-19T16:00:00Z","relpermalink":"/talk/collectivedynamics/","section":"talk","summary":"Single-Agent Policies for Multi-Agent Persistent Surveillance.","tags":[],"title":"Ignorance is bliss - the role of noise and heterogeneity in training and deployment of Single Agent Policies for the Multi-Agent Persistent Surveillance Problem","type":"talk"},{"authors":["Thomas E. Kent","Arthur G. Richards"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"56ef2271bde13bec07b0836cd2e5942a","permalink":"https://tomekent.com/publication/kent-2019-a/","publishdate":"2020-02-07T12:22:13.596989Z","relpermalink":"/publication/kent-2019-a/","section":"publication","summary":"The Travelling Salesman and its variations are some of the most well known NP hard optimisation problems. This paper looks to use both centralised and decentralised implementations of Evolutionary Algorithms (EA) to solve a dynamic variant of the Multi-Agent Travelling Salesman Problem (MATSP). The problem is dynamic, requiring an on-line solution, whereby tasks are completed during simulation with new tasks added and completed ones removed. The problem is allocating an active set of tasks to a set of agents whilst simultaneously planning the route for each agent. The allocation and routing are closely coupled parts of the same problem making it difficult to decompose, instead this paper uses multiple populations with well defined interactions to exploit the problem structure. This work attempts to align the real world implementation demands of a decentralised solution, where agents are far apart and have communication limits, to that of the structure of the multi-demic EA solution process, ultimately allowing decentralised parts of the problem to be solved `on board' agents and allow for robust communication and exchange of tasks.","tags":["Allocation and Routing","Decision Making","Distributed problem solving","Evolutionary Algorithms","Multi Agent Travelling Salesman"],"title":"Decentralised Multi-Demic Evolutionary Approach to the Dynamic Multi-Agent Travelling Salesman Problem","type":"publication"},{"authors":["Thomas E. Kent","Arthur G. Richards"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a09cded5865c5b46b48d5297dfb4f9bb","permalink":"https://tomekent.com/publication/kent-2019/","publishdate":"2020-02-07T12:22:13.598038Z","relpermalink":"/publication/kent-2019/","section":"publication","summary":"","tags":null,"title":"Decentralised multi-demic evolutionary approach to the dynamic multi-agent travelling salesman problem","type":"publication"},{"authors":["Thomas E. Kent"],"categories":null,"content":"","date":1533654000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533654000,"objectID":"3d48716b37ac15db091367ef4bebe58d","permalink":"https://tomekent.com/talk/tbphase/","publishdate":"2018-08-07T16:00:00Z","relpermalink":"/talk/tbphase/","section":"talk","summary":"We look at the use of bi-level optimisation techniques to solve a number of standard assignment and routing problems.","tags":[],"title":"Bi-Level Optimisation for Assignment and Routing Problems","type":"talk"},{"authors":["Thomas E. Kent","Arthur G. Richards"],"categories":null,"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443657600,"objectID":"354ef526c49a9f3bd33ee80e83f7f9bb","permalink":"https://tomekent.com/publication/kent-2015/","publishdate":"2020-02-07T12:22:13.599902Z","relpermalink":"/publication/kent-2015/","section":"publication","summary":"","tags":null,"title":"Analytic Approach to Optimal Routing for Commercial Formation Flight","type":"publication"},{"authors":["Thomas E Kent"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"6f67080142cd7bbcce490fb3d1cd8d81","permalink":"https://tomekent.com/publication/kent-2015-a/","publishdate":"2020-02-07T12:22:13.598863Z","relpermalink":"/publication/kent-2015-a/","section":"publication","summary":"This thesis investigates the notion of fuel-reduction through formation flight for com- mercial aircraft, addressing the problems of global routing and assignment. A two stage centralised approach is presented, firstly, assuming a reduction in observed cost by flying in formation the routes, including rendezvous and break points, are calcu- lated to minimise a total cost. The interconnected assignment problem then takes a set of flights, their possible formations and corresponding costs and optimally allo- cates them into a cost-minimising formation fleet. An analytic geometric approach is used to develop a scalable methodology for the formation routing problem enabling the quick calculation of costs. The rapid evaluation allows the large scale fleet assignment problem to be solved via a Mixed Integer Linear Program in reasonable time. A Transatlantic case study shows possible formation fuel savings against solo flight of around 8.7% and 13.1% for formations up to size two and three respectively. Further case studies of three distinct sets of flights show that encouraging levels of saving can still be achieved by flights with varied distances, geographical locations and formation drag-reduction levels. For the more complex task of routing through wind, results show that the analytic approach can act as a reasonable estimate to the assignment problem, allowing higher- fidelity and computationally more intensive routing methods to be introduced via a post-process, significantly reducing solve time. Methods for mitigating the impact of uncertainty in aircraft take-off times are ex- plored, where a state-space approach, solved using value iteration, can provide optimal speed-policies for aircraft to follow for any possible realisation of delay. Additionally portfolio optimisation provides a method for formations to be assigned to simultane- ously maximise reward and minimise the associated risk. Finally the calculation of efficient frontiers allows matching of reward to desired levels of risk-aversion.","tags":null,"title":"Optimal Routing and Assignment for Commercial Formation Flight","type":"publication"},{"authors":["Thomas E. Kent","Arthur G. Richards"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"9cc4191bde3b827b15b0315d0c7e1be0","permalink":"https://tomekent.com/publication/kent-2014-a/","publishdate":"2020-02-07T12:22:13.599544Z","relpermalink":"/publication/kent-2014-a/","section":"publication","summary":"","tags":["Delay","Dynamic Programming","Formation Flight","Routing","Value Iteration"],"title":"Accounting for the effect of ground delay on commercial formation flight","type":"publication"},{"authors":["Thomas E. Kent","Arthur G. Richards"],"categories":null,"content":"","date":1375315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375315200,"objectID":"fab199e1ffee9e478ac6948707de5e74","permalink":"https://tomekent.com/publication/kent-2013/","publishdate":"2020-02-07T12:22:13.599173Z","relpermalink":"/publication/kent-2013/","section":"publication","summary":"This paper explores an extension to a geometric approach of finding optimal routes for commercial formation flight. An adaption of the Breguet range equation, alongside specific aircraft characteristics, is used to represent realistic aircraft and underlying changes in weight as fuel is burnt off. Weighting schemes, for both nominal and differential rates of fuel burn, are introduced and compared. Finally a method for finding wind-optimal routes in a formation flight paradigm is developed in order to assess the effectiveness of a geometric estimate for the formation pair assignment problem. Using the geometric method to allocate formation pairs is shown to offer good performance for solutions with a significant reduction in computation time against all possible wind-optimal formation routes.","tags":null,"title":"On Optimal Routing for Commercial Formation Flight","type":"publication"},{"authors":["Thomas E. Kent","Arthur G. Richards"],"categories":null,"content":"","date":1343779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1343779200,"objectID":"2e7df3c234c7b0637f7497632073c623","permalink":"https://tomekent.com/publication/kent-2012/","publishdate":"2020-02-07T12:22:13.597589Z","relpermalink":"/publication/kent-2012/","section":"publication","summary":"This paper explores a geometric approach to finding optimal routes for commercial formation flight. A weighted extension of the classical Fermat point problem is used to develop an analytic solution to finding optimal routes, thereby reducing the complexity of the problem and enabling a quick evaluation. We then construct a method to decouple origin and destination nodes creating a vertex from which the route projects, along with loci of possible points of formation. This implementation enables us to take lists of routes and efficiently decompose them to find the optimal locations for flights to meet, fly in formation and then break away and continue on their solo paths. We look at a case study of creating formations from 210 transatlantic flights for a fleet size of up to 2, resulting in overall global approximate total fuel savings of 8.6%. Furthermore we explore heuristic methods to finding solutions when creating larger fleet size formations, indicating savings surpassing 10%. © 2011 by the American Institute of Aeronautics and Astronautics, Inc.","tags":null,"title":"A Geometric Approach to Optimal Routing for Commercial Formation Flight","type":"publication"}]