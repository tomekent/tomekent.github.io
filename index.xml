<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Thomas E. Kent</title>
    <link>https://tomekent.com/</link>
      <atom:link href="https://tomekent.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Thomas E. Kent</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 14 Sep 2020 15:00:00 +0100</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Thomas E. Kent</title>
      <link>https://tomekent.com/</link>
    </image>
    
    <item>
      <title>Single-Agent Policies for the Multi-Agent Persistent Surveillance Problem via Artificial Heterogeneity</title>
      <link>https://tomekent.com/talk/eumas2020/</link>
      <pubDate>Mon, 14 Sep 2020 15:00:00 +0100</pubDate>
      <guid>https://tomekent.com/talk/eumas2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Single-Agent Policies for the Multi-Agent Persistent Surveillance Problem via Artificial Heterogeneity</title>
      <link>https://tomekent.com/publication/kent-2020a/</link>
      <pubDate>Mon, 14 Sep 2020 11:38:05 +0100</pubDate>
      <guid>https://tomekent.com/publication/kent-2020a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Test</title>
      <link>https://tomekent.com/slides/test/</link>
      <pubDate>Wed, 26 Feb 2020 09:46:05 +0000</pubDate>
      <guid>https://tomekent.com/slides/test/</guid>
      <description>&lt;h1 id=&#34;title&#34;&gt;Title&lt;/h1&gt;
&lt;p&gt;Author Name&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;p&gt;&amp;hellip; Hey&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if x&amp;lt;x_lim:
    run_me(x)
else:
    run_you(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;body&gt;
       &lt;style type=&#34;text/css&#34;&gt;
           #header-left {
               position: absolute;
               top: 0%;
               left: 0%;
           }
           #header-right {
               position: absolute;
               top: 0%;
               right: 0%;
           }
           #footer-left {
               position: absolute;
               bottom: 0%;
               left: 0%;
           }
       &lt;/style&gt;
&lt;pre&gt;&lt;code&gt;   &amp;lt;div id=&amp;quot;hidden&amp;quot; style=&amp;quot;display:none;&amp;quot;&amp;gt;
       &amp;lt;div id=&amp;quot;header&amp;quot;&amp;gt;
           &amp;lt;div id=&amp;quot;header-left&amp;quot;&amp;gt;…&amp;lt;/div&amp;gt;
           &amp;lt;div id=&amp;quot;header-right&amp;quot;&amp;gt;…&amp;lt;/div&amp;gt;
           &amp;lt;div id=&amp;quot;footer-left&amp;quot;&amp;gt;…&amp;lt;/div&amp;gt;
       &amp;lt;/div&amp;gt;
   &amp;lt;/div&amp;gt;

   &amp;lt;div class=&amp;quot;reveal&amp;quot;&amp;gt;
       &amp;lt;div class=&amp;quot;slides&amp;quot;&amp;gt;
           …
       &amp;lt;/div&amp;gt;
   &amp;lt;/div&amp;gt;

   &amp;lt;script src=&amp;quot;reveal.js/lib/js/head.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
   &amp;lt;script src=&amp;quot;reveal.js/js/reveal.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

   &amp;lt;script&amp;gt;
       …
   &amp;lt;/script&amp;gt;

   &amp;lt;script src=&amp;quot;jquery/jquery-2.1.3.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

   &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
       var header = $(&#39;#header&#39;).html();
       if ( window.location.search.match( /print-pdf/gi ) ) {
           $(&#39;.slides &amp;gt; section&#39;).prepend(header);
       }
       else {
           $(&#39;.slides&#39;).prepend(header);
       }
   &amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
   &lt;body&gt;
</description>
    </item>
    
    <item>
      <title>Continuous Control, DDPG and Neuron saturation</title>
      <link>https://tomekent.com/post/neuron_saturation/</link>
      <pubDate>Thu, 20 Feb 2020 12:10:03 +0000</pubDate>
      <guid>https://tomekent.com/post/neuron_saturation/</guid>
      <description>&lt;h2 id=&#34;never-work-with-angles&#34;&gt;Never work with angles&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m sure if W.C. Fields was alive today and interested in machine learning he might have updated his maxim of never working with children or animals to include angles.&lt;/p&gt;
&lt;p&gt;Many off the shelf RL algorithms seem like straightforward tools to replicate state of the art results on your own specific problem domain. Deep neural nets have shown great promise of &amp;lsquo;learning&amp;rsquo; complex tasks such as computer vision and continuous control problems but to adapt them can be challenging and require greater knowledge of the underlying mathematics and Neural Network structures to debug problems - often parameter tuning by trial and error.&lt;/p&gt;
&lt;p&gt;Lets have a look at Deep Deterministic Policy Gradient (DDPG) and see how well a Deep Neural Network, approximating a policy function, can perform at an angle based continuous control problem.&lt;/p&gt;
&lt;h2 id=&#34;continuous-action-control-choice-for-persistent-surveillance&#34;&gt;Continuous Action Control choice for Persistent Surveillance&lt;/h2&gt;
&lt;p&gt;Some of my current work is looking at training/designing polices for multiple agents to perform persistent surveillance (similar to coverage/patrolling problems.). The aim is for agents to maximise some metric, &amp;lsquo;hex score&amp;rsquo;, that represents how well the world&amp;rsquo;s current &amp;lsquo;level of surveillance&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;As an agent enters a hex it observes it and the score shoots up, then all hexes currently not directly observed have their score decay (with some half life).&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/neuron_saturation/RL_flow_hu19952ed86cd2e4a82b64167dd55dabdb_254460_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Reinforcement Learning state, action reward flow&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/neuron_saturation/RL_flow_hu19952ed86cd2e4a82b64167dd55dabdb_254460_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1388&#34; height=&#34;560&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Reinforcement Learning state, action reward flow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Our observation, and thus our input to our Neural Network are the hex scores around the agent. This is feed into our network in order to choose the optimal action (direction to move) for the agent to take.&lt;/p&gt;
&lt;h3 id=&#34;single-neuron-output&#34;&gt;Single Neuron Output&lt;/h3&gt;
&lt;p&gt;The standard off the shelf DDPG algorithm for a continuous control task has a single neuron output.





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/neuron_saturation/NN_PS_1D_huce17e29cee23123115ae54ee0486ada5_96940_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Deep Neural Net with 1 output neuron&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/neuron_saturation/NN_PS_1D_huce17e29cee23123115ae54ee0486ada5_96940_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;807&#34; height=&#34;455&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Deep Neural Net with 1 output neuron
  &lt;/figcaption&gt;


&lt;/figure&gt;

We replicate this and choose our output to be a single neuron, with a $\tanh$ activation function. This activation value which lies between $[-1, 1]$ can be multiplied by $\pi$ to recover an angle $\theta \in [-\pi, +\pi]$ - this is the direction the agent should move.





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/neuron_saturation/tanh_hub637aef497ecec60f3206fc452d24de9_7624_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Activation function $\tanh$&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/neuron_saturation/tanh_hub637aef497ecec60f3206fc452d24de9_7624_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;490&#34; height=&#34;270&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Activation function $\tanh$
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;trained-ddpg-agent&#34;&gt;Trained DDPG Agent&lt;/h3&gt;
&lt;p&gt;After 2000 runs of 600 time steps the DDPG policy has successfully converged and produces &lt;em&gt;half decent&lt;/em&gt; results.
&lt;img src=&#34;result_animation1.gif&#34; alt=&#34;Agent performing persistent surveillance in a hex world&#34;&gt;
It runs around &lt;em&gt;heating&lt;/em&gt; those hexes up reaching decent scores, so lets see if we can improve it.&lt;/p&gt;
&lt;h2 id=&#34;rose-plots-and-neuron-saturation&#34;&gt;Rose Plots and Neuron Saturation&lt;/h2&gt;
&lt;p&gt;While it appears to work fairly well, if we use a rose-plot to show action choices over an entire episode we can see an issue. This rose plot indicates frequency of angle chosen:





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/neuron_saturation/action_choices_1D_hu3f6a3828a8e3aae45e42766374c9f5b2_68149_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Action choices rose plot for 3 policies&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/neuron_saturation/action_choices_1D_hu3f6a3828a8e3aae45e42766374c9f5b2_68149_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;544&#34; height=&#34;196&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Action choices rose plot for 3 policies
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the left we have completely random action selection;&lt;/li&gt;
&lt;li&gt;In the middle a discrete 6 direction choice made by a simple greedy heuristic algorithm;&lt;/li&gt;
&lt;li&gt;On the right a Deep Deterministic Policy Gradient (DDPG) as outline above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What we hope for is something that produces a better, more continuous version of the heuristic (middle), instead what we get is good old &lt;strong&gt;neuron saturation&lt;/strong&gt;. The problem here is the $\tanh$ activation function (and a problem that exists in all activation functions) saturates at +1 or -1, the asymptotes, and as a result so too does our action selection at $+\pi$ or $-\pi$ respectively.&lt;/p&gt;
&lt;h3 id=&#34;double-neuron-output&#34;&gt;Double Neuron Output&lt;/h3&gt;
&lt;p&gt;That said, issues with working with angles and its discontinuities are well documented. One approach to fix this is to add more output neurons. We opt to add a second neuron and parametrise the action angle so that instead of multiplying the activation by $\pi$ we use it to represent $\sin(\theta)$ and $\cos(\theta)$ (which are easily converted into $\theta =\arctan(\sin(\theta)/\cos(\theta))$).





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/neuron_saturation/NN_PS_2D_hu0f4b464f92eaed595070d2fece475984_107298_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Deep Neural Net with 2 output neuron&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/neuron_saturation/NN_PS_2D_hu0f4b464f92eaed595070d2fece475984_107298_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;902&#34; height=&#34;455&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Deep Neural Net with 2 output neuron
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/neuron_saturation/action_choices_2D_hu5e26145098f5c49f58806e948031233c_73890_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Action choices rose plot: [Random, discrete heuristic, DDPG]&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/neuron_saturation/action_choices_2D_hu5e26145098f5c49f58806e948031233c_73890_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;544&#34; height=&#34;196&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Action choices rose plot: [Random, discrete heuristic, DDPG]
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This balances out some of the neuron saturation but also seems to instead now saturate at the &amp;lsquo;corners&amp;rsquo; [(1,1), (1,-1), (-1,-1), (-1,1)]&lt;/p&gt;
&lt;h2 id=&#34;multi-agent-deployment&#34;&gt;Multi Agent Deployment&lt;/h2&gt;
&lt;p&gt;Some of our most recent work looks at whether we can simply train single agent policies like above and deploy them to multi agent problems - &lt;em&gt;without Multi-Agent RL&lt;/em&gt;. It turns out we can, but there are catches. I recently gave a talk called 
&lt;a href=&#34;talk/collectivedynamics/&#34;&gt;Ignorance is bliss - the role of noise and Heterogeneity in training and deployment of Single Agent Policies for the Multi-Agent Persistent Survellance Problem7&lt;/a&gt;
 which goes into some more detail.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;result_animation_3agents.gif&#34; alt=&#34;Agent performing persistent surveillance in a hex world&#34;&gt;&lt;/p&gt;
&lt;p&gt;A paper which is currently under review, &lt;em&gt;Single-Agent Polices for the Multi-Agent Persistent Surveillance
Problem via Artificial Heterogeneity&lt;/em&gt;, describes how deploying multiple heterogeneous agents can cause undesirable emergent clustering behaviour. So stay tuned for that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolutionary Algorithms for the Multi Agent Travelling Salesman Problem</title>
      <link>https://tomekent.com/post/multiagent-travellingsalesmanproblem/</link>
      <pubDate>Fri, 07 Feb 2020 22:55:39 +0000</pubDate>
      <guid>https://tomekent.com/post/multiagent-travellingsalesmanproblem/</guid>
      <description>&lt;p&gt;This is a summary of some work published last year at the 
&lt;a href=&#34;https://gecco-2019.sigevo.org/index.html/HomePage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019 Genetic and Evolutionary Computation Conference&lt;/a&gt;
. Where we are looking to solve routing and allocation problems and trying to find ways to balance the increase in computational demand required when increasing the number of numbers of agents&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/bars_hu1a0786e2572b06a63892b99ae87e446b_37087_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Trade-off between optimisation factors&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/bars_hu1a0786e2572b06a63892b99ae87e446b_37087_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;502&#34; height=&#34;292&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Trade-off between optimisation factors
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we want to solve increasingly complex problems, then we might either need more compute, or we sacrifice solution quality - &lt;strong&gt;unless we can be smart about the way we decompose the problem&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;general-problem&#34;&gt;General Problem&lt;/h2&gt;
&lt;p&gt;There are a number of real world problems that require several agents to visit areas of interest, complete tasks and travel between them. These typically include problems such as surveillance, exploration or search and rescue.&lt;/p&gt;
&lt;p&gt;Here we define a fairly standard Multi-Agent Travelling Salesman Problem:&lt;/p&gt;
&lt;h4 id=&#34;setup&#34;&gt;Setup&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Allocating active set of tasks to a set of agents&lt;/li&gt;
&lt;li&gt;Simultaneously planning agents&amp;rsquo; optimal routes&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;caveats&#34;&gt;Caveats&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Allocation and routing are closely coupled&lt;/li&gt;
&lt;li&gt;Dynamic simulated environment&lt;/li&gt;
&lt;li&gt;Need to consider real world implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What we have learnt from our work on this is that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;We should align real-world Multi-Agent constraints with the structuring of our optimisation technique&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;evolutionary-algorithm-for-matsp&#34;&gt;Evolutionary Algorithm for MATSP&lt;/h2&gt;
&lt;p&gt;Our aim is to move away from a single centralised solution to the MATSP. Instead we want to utilise the fact that these agents exist in a world that can be compartmentalised. There are real world constraints, such as geographical seperation that can be used to our advantage.&lt;/p&gt;
&lt;p&gt;We use the population-distribution island-model, where the global population is divided into a number of demes (distinct populations) and referred to as the Multi-Demic Evolutionary Algorithm (MDEA). Communications between these demes allow for individuals to migrate between them at pre-defined intervals. These demes are structured to align with real world execution of a MATSP where tasks are distributed amongst multiple agents and are completed independently.&lt;/p&gt;
&lt;h3 id=&#34;matsp-problem-statement&#34;&gt;MATSP Problem Statement&lt;/h3&gt;
&lt;p&gt;Here we present what&amp;rsquo;s known as thethe three-index flow-based formulation
First define the indexes $i$ and $j$ to denote a task from the set T
of tasks 1 to N , the set A of agents from 1 to M and the matrix
c i ja to denote the cost of agent a travelling from task i to task j.
Additionally we define the three-index binary decision variable:&lt;/p&gt;
&lt;p&gt;First define the indexes $i$ and $j$ to denote a task from the set $T$ of tasks 1 to $N$, the set $A$ of agents from 1 to $M$ and the matrix $c_{ija}$ to denote the cost of agent $a$ travelling from task $i$ to task $j$. Additionally we define the three-index binary decision variable:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation*}
x_{ija} =
\begin{cases}
1 &amp;amp; \text{if agent $a$ visits task $j$ after $i$,}\\\&lt;br&gt;
0 &amp;amp; \text{otherwise}
\end{cases}
\end{equation*}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\min_{x_{ija}} &amp;amp; \sum_{i = 1}^{N} \sum_{j = 1}^{N} \sum_{a = 1}^{M} c_{ija} x_{ija} \\\&lt;br&gt;
&amp;amp; \sum_{i = 1}^{N}x_{ipa} - \sum_{j = 1}^{N}x_{pja} = 0, \text{  }a \in A, p \in T \label{MATSP_flow} \\\&lt;br&gt;
&amp;amp; \sum_{j = 1}^{N}x_{1ja} = 1, \text{  } \forall a \in A \label{MATSP_one_agent}\\\&lt;br&gt;
&amp;amp; u_i - u_j + N \sum_{a = 1}^{M} x_{ija}  \leq N -1, \text{  } \forall i \neq j \neq 1 \label{MATSP_subtour}\\\&lt;br&gt;
&amp;amp; x_{ija} \in {0,1} \text{   } \forall i,j,a
\end{align}
$$&lt;/p&gt;
&lt;!--	\text{s.t.}&amp; \sum_{i = 1}^{N}\sum_{a = 1}^{M} x_{ija} = 1, \text{  } \forall j \label{MATSP_one_task}\\
						 &amp; \sum_{i = 1}^{N}x_{ipa} - \sum_{j = 1}^{N}x_{pja} = 0, \text{  }a \in A, p \in T \label{MATSP_flow} \\
						 &amp; \sum_{j = 1}^{N}x_{1ja} = 1, \text{  } \forall a \in A \label{MATSP_one_agent}\\
						 &amp; u_i - u_j + N \sum_{a = 1}^{M} x_{ija}  \leq N -1, \text{  } \forall i \neq j \neq 1 \label{MATSP_subtour}\\
						 &amp; x_{ija} \in \{0,1\} \text{   } \forall i,j,a --&gt;
&lt;p&gt;The objective, is to minimize the total cost of all the agents travelling between the assigned tasks. The next constraints ensure that each task is visited only once while the flow conservation constraints state that once an agent visits a task then they must also depart from it. The &amp;lsquo;one-agent&amp;rsquo; constraints ensure that each agent is used only once and the sub-tour elimination constraints are used, where $u$ are additional non-negative auxiliary decision variables, with $u_i$ corresponding to the ith task, known as &amp;lsquo;node potentials&amp;rsquo;.&lt;/p&gt;
&lt;h3 id=&#34;evolutionary-approach-for-matsp&#34;&gt;Evolutionary Approach for MATSP&lt;/h3&gt;
&lt;h4 id=&#34;3-main-evolutionary-stages&#34;&gt;3 main Evolutionary Stages&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialisation&lt;/strong&gt; - creating an starting population for which to evolve;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reproduction&lt;/strong&gt; - carrying out evolutionary operators such as crossover, mutation and improvement heuristics to produce offspring;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Selection&lt;/strong&gt; - taking individuals from both the main population and from the offspring to produce the new population;&lt;/li&gt;
&lt;/ol&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/chromosome_hu75c523082cc201b794c3cc68caa1fbd4_21445_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Chromosome representation of route structure&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/chromosome_hu75c523082cc201b794c3cc68caa1fbd4_21445_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;840&#34; height=&#34;190&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Chromosome representation of route structure
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;crossover-operator-to-create-offspring-from-combining-features-of-the-two-parent-solutions&#34;&gt;Crossover operator to create offspring from combining features of the two parent solutions&lt;/h4&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Crossover_SBX_hu4e67604dabaa4492804c771768d1fee2_43848_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;SBX Crossover operator&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Crossover_SBX_hu4e67604dabaa4492804c771768d1fee2_43848_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;840&#34; height=&#34;388&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    SBX Crossover operator
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;mutation-operators-to-create-slightly-different-offspring-from-one-parent-solutions&#34;&gt;Mutation operators to create slightly different offspring from one parent solutions&lt;/h4&gt;
&lt;p&gt;




  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Mutation_reallocate_hudf00e8223de216f7cf908e9bc7e9cdb1_32459_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mutation operator - reallocate tasks&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Mutation_reallocate_hudf00e8223de216f7cf908e9bc7e9cdb1_32459_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;513&#34; height=&#34;444&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Mutation operator - reallocate tasks
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Mutation_swap_hu9ece16d2204ae39d5a6d703dcf60ef23_29885_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mutation operator - Swap Tasks&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Mutation_swap_hu9ece16d2204ae39d5a6d703dcf60ef23_29885_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;513&#34; height=&#34;431&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Mutation operator - Swap Tasks
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;improvement-operator-to-try-to-quickly-improve-a-solution-using-2-opt&#34;&gt;Improvement operator to try to quickly improve a solution using 2 opt&lt;/h4&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Improvement_hu686ebed896d92e654f8c05433e0f7a0f_37986_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Improvement operator - 2 opt swap&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Improvement_hu686ebed896d92e654f8c05433e0f7a0f_37986_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;675&#34; height=&#34;482&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Improvement operator - 2 opt swap
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;aim-multi-demic-evolutionary-algorithm&#34;&gt;Aim: Multi-demic Evolutionary Algorithm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exploit problem structure, aligning real-world implementation demands&lt;/li&gt;
&lt;li&gt;Decentralised solution with Communication&lt;/li&gt;
&lt;li&gt;Use multiple populations (or demes)&lt;/li&gt;
&lt;li&gt;With well-defined agent-population interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each agent has population structures for &amp;lsquo;thinking about&amp;rsquo; intereactions with other agents. A population &amp;lsquo;A-B&amp;rsquo; that exists onboard agent A is constantly evolved and updated. If new tasks arise these are incorporated, if agent A and agent B come in communiction range then they can &amp;lsquo;pool&amp;rsquo; their A-B and B-A populations and prune any that are now invalid.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Populations_flow_hub51ca700d509b4363b269ab48a4ff839_201338_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Population Structures&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/Populations_flow_hub51ca700d509b4363b269ab48a4ff839_201338_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1370&#34; height=&#34;594&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Population Structures
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The objective function, total distance travelled, shown in the below figure, clearly shows that as the communications restriction is gradually lifted the total distances of the dMDEA results tends to the cMDEA, notably, any communication radius of 125 or greater either matches or outperforms the EA.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/dist_hu3d293bd923243efe5a680a8515147495_29125_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Communication range vs Total Distance&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/dist_hu3d293bd923243efe5a680a8515147495_29125_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Communication range vs Total Distance
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In addition, as communication range is increased the agents spend more time evolving the demes corresponding to nearby agents and thus the linear runtime increases. The next figure shows the relationship between the communication radius and thus the number of other agents to consider and the resulting run-time. Therefore there is a clear trade-off decision between ability to communicate, and thus agents you should consider, and run-time.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/sim_t_hu04edef47508d77f92ae2e272a88536da_25911_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Communication range vs Total Sim time&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/sim_t_hu04edef47508d77f92ae2e272a88536da_25911_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Communication range vs Total Sim time
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;demos&#34;&gt;Demos&lt;/h2&gt;
&lt;h3 id=&#34;decentralised---homogeneous-comms&#34;&gt;Decentralised - Homogeneous Comms&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/uMYGhZartHw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;decentralised---heterogeneous-comms&#34;&gt;Decentralised - Heterogeneous Comms&lt;/h3&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/2TYcICFPSTE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;but-i-was-after-a-free-lunch&#34;&gt;But I was after a free lunch&lt;/h2&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/cycle_hu53326aaf020c0bab91f1cc0e7e3106bc_22496_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Trade-off between optimisation factors&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/cycle_hu53326aaf020c0bab91f1cc0e7e3106bc_22496_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;363&#34; height=&#34;203&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Trade-off between optimisation factors
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Sadly that&amp;rsquo;s really the issue here, in many optimisation problems, but particularly in heuristic iterative approaches such as EAs, there is a trade off between problem complexity, runtime and quality of solution (Figure 4). If you want to keep your compute time the same, i.e. limit generations and population size, but need to solve a harder problem, i.e. &lt;strong&gt;one with more agents&lt;/strong&gt;, you will need to sacrifice solution quality.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/bars_hu1a0786e2572b06a63892b99ae87e446b_37087_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Trade-off between optimisation factors&#34;&gt;


  &lt;img data-src=&#34;https://tomekent.com/post/multiagent-travellingsalesmanproblem/bars_hu1a0786e2572b06a63892b99ae87e446b_37087_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;502&#34; height=&#34;292&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Trade-off between optimisation factors
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;read-the-full-paper&#34;&gt;Read the full paper&lt;/h2&gt;
&lt;p&gt;If you want to read in a bit more detail then you can find the full paper here:
&lt;a  href=&#34;https://tomekent.com/publication/kent-2019-a&#34;   class=&#34;btn btn-outline-primary my-1 mr-1&#34; rel=&#34;noopener&#34;&gt;
      &lt;i class=&#34;fas fa-far fa-file-alt pr-1 fa-fw&#34;&gt;&lt;/i&gt;
      
      Kent 2019
&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Co2 Emissions Reduction via formation flight</title>
      <link>https://tomekent.com/post/co2-formation/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/post/co2-formation/</guid>
      <description>&lt;h2 id=&#34;co2-emissions&#34;&gt;Co2 Emissions&lt;/h2&gt;
&lt;p&gt;I had a quick look back over some of my PhD work in preparation of a new paper I&amp;rsquo;m writing. I found it slightly mad that I never really included any numbers on Co2 emissions.&lt;/p&gt;
&lt;p&gt;So I&amp;rsquo;ve been working on some under the assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The emissions index of CO 2 per kg of fuel burnt is estimated to be roughly 3.16 
&lt;a href=&#34;https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19920009879.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;A rough estimate of 600 dollars per metric tonne of fuel 
&lt;a href=&#34;https://www.iata.org/en/publications/economics/fuel-monitor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;singapore-airlines&#34;&gt;Singapore Airlines&lt;/h2&gt;
&lt;p&gt;Take Singapore Airlines as an example, on a single day some &lt;strong&gt;200 flights could potentially fly in 100 formations pairs&lt;/strong&gt;.
This would save on average 6.7% of fuel against those same flights flying solo. This works out at 610 tonnes of fuel saved, and as a direct result would burn roughly 1,929 tonnes fewer of Co2. Not only that but they&amp;rsquo;d save 366k in the process but simply not buying the fuel.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s for a single day.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s like planting &lt;strong&gt;704,085 trees a year&lt;/strong&gt; and getting paid $133m for it.&lt;/p&gt;
&lt;h3 id=&#34;equivalent-of-planting-80-per-hour&#34;&gt;Equivalent of planting 80 per hour:&lt;/h3&gt;
&lt;p&gt;🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳🌳&lt;/p&gt;
&lt;h3 id=&#34;and-getting-paid-190-per-tree-at-the-same-time&#34;&gt;And getting paid $190 per tree at the same time&lt;/h3&gt;
&lt;p&gt;💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰💰&lt;/p&gt;
&lt;h3 id=&#34;want-to-find-out-more&#34;&gt;Want to find out more?&lt;/h3&gt;
&lt;p&gt;Take a look a page I put together a while back to explore the datasets:
&lt;a href=&#34;http://tomekent.com/FormationFlight/&#34;&gt;http://tomekent.com/FormationFlight/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;found-this-interesting-consider-sharing-it-&#34;&gt;Found this interesting? Consider sharing it 🙌&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>A Connected Autonomous Vehicle Testbed: Capabilities, Experimental Processes and Lessons Learned</title>
      <link>https://tomekent.com/publication/kent-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ignorance is bliss - the role of noise and heterogeneity in training and deployment of Single Agent Policies for the Multi-Agent Persistent Surveillance Problem</title>
      <link>https://tomekent.com/talk/collectivedynamics/</link>
      <pubDate>Sat, 19 Oct 2019 15:00:00 +0000</pubDate>
      <guid>https://tomekent.com/talk/collectivedynamics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decentralised Multi-Demic Evolutionary Approach to the Dynamic Multi-Agent Travelling Salesman Problem</title>
      <link>https://tomekent.com/publication/kent-2019-a/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2019-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decentralised Multi-Demic Evolutionary Approach to the Dynamic Multi-Agent Travelling Salesman Problem</title>
      <link>https://tomekent.com/publication/kent-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bi-Level Optimisation for Assignment and Routing Problems</title>
      <link>https://tomekent.com/talk/tbphase/</link>
      <pubDate>Tue, 07 Aug 2018 15:00:00 +0000</pubDate>
      <guid>https://tomekent.com/talk/tbphase/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analytic Approach to Optimal Routing for Commercial Formation Flight</title>
      <link>https://tomekent.com/publication/kent-2015/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimal Routing and Assignment for Commercial Formation Flight</title>
      <link>https://tomekent.com/publication/kent-2015-thesis/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2015-thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accounting for the effect of ground delay on commercial formation flight</title>
      <link>https://tomekent.com/publication/kent-2014/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2014/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On Optimal Routing for Commercial Formation Flight</title>
      <link>https://tomekent.com/publication/kent-2013/</link>
      <pubDate>Thu, 01 Aug 2013 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2013/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Geometric Approach to Optimal Routing for Commercial Formation Flight</title>
      <link>https://tomekent.com/publication/kent-2012/</link>
      <pubDate>Wed, 01 Aug 2012 00:00:00 +0000</pubDate>
      <guid>https://tomekent.com/publication/kent-2012/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
